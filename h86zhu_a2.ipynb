{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOU_a2 (v1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q1: Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Your answer here (double-click)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To help you with $\\LaTeX$, and to show you my expectations, here is a sample taken from the lecture notes, taken from the 3rd and 4th page of the notes entitled \"Error Backpropagation\". It has nothing to do with the solution to this question, but just demonstrates some of the features of $\\LaTeX$. Notice how I include English statments to guide the reader through the derivation.\n",
    "\n",
    "<a target=_new href=\"http://detexify.kirelabs.org/classify.html\">This web page</a> is very handy for identifying $\\LaTeX$ symbols.\n",
    "\n",
    "---\n",
    "More generally, for $\\vec{x} \\in \\mathbb{R}^X$, $\\vec{h} \\in \\mathbb{R}^H$, and $\\vec{y} \\in \\mathbb{R}^Y$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial \\alpha_i}\n",
    "  &= \\frac{d h_i}{d \\alpha_i} \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "  \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right] \\cdot\n",
    "  \\left[ \\frac{\\partial E}{\\partial \\beta_1} \\ \\cdots \\ \\frac{\\partial E}{\\partial \\beta_Y} \\right] \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "   \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right]\n",
    "   \\left[ \\begin{array}{c}\n",
    "     \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "     \\vdots \\\\\n",
    "     \\frac{\\partial E}{\\partial \\beta_Y} \\end{array} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "Thus, for all elements,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\alpha_H}\n",
    "\\end{array} \\right] &=\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{d h_1}{d \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{d h_H}{d \\alpha_H}\n",
    "\\end{array} \\right]\n",
    "\\odot\n",
    "\\left[ \\begin{array}{ccc}\n",
    "  M_{11} & \\cdots & M_{Y1} \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  M_{1H} & \\cdots & M_{YH}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\beta_Y}\n",
    "\\end{array} \\right] \\\\\n",
    "%\n",
    "\\frac{\\partial E}{\\partial \\vec{\\alpha}} &=\n",
    "\\frac{d \\vec{h}}{d \\vec{\\alpha}} \\odot M^\\mathrm{T}\n",
    "\\frac{\\partial E}{\\partial \\vec{\\beta}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the rule of derivative, we can re-write the expression for sigma.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d\\sigma(z)}{dz}\n",
    "  &= -\\frac{1}{(1+e^{-z})^2} * e^{-z}\\\\\n",
    "  &= \\frac{e^{-z}}{(1+e^{-z})^2}\\\\\n",
    "  &= \\frac{1}{1+e^{-z}} * \\frac{e^{-z}}{1+e^{-z}}\\\\\n",
    "  &= \\frac{1}{1+e^{-z}} * [1 - \\frac{1}{1+e^{-z}}]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now if we plug back $\\delta(z)$ expression: we have the following\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d\\sigma(z)}{dz}\n",
    "  &= \\sigma(z) * (1 - \\sigma(z))\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q2: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Your answer here.\n",
    "First we involve an itermediate item - $y_k$ in the derivative and we can then split into two cases:\n",
    "\n",
    "Since j is an input current to a specific node j, we have when k = j(1 senario) and k != j (K - 1 senarios). Thus:\n",
    "\n",
    "Assume k != j\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_j}\n",
    "  &= \\sum_{k=1, i \\neq j}^{K} \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial z_j} + \\frac{\\partial E}{\\partial y_j}\\frac{\\partial y_j}{\\partial z_j} * 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For j = k:\n",
    "$$\\frac{\\partial E}{\\partial y_j}\\frac{\\partial y_j}{\\partial z_j} = -t_j * \\frac{\\sum_{i=1}^{K} e^{z_i}}{e^{z_k}} * \\frac{e^{z_k} * (e^{z_1}+...+e^{z_{j-1}}+e^{z_{j+1}}+...+e^{z_K})}{(\\sum_{i=1}^{K} e^{z_i})^2}  \\\\\n",
    "= -t_j * \\frac{(e^{z_1}+...+e^{z_{j-1}}+e^{z_{j+1}}+...+e^{z_K})}{\\sum_{i=1}^{K} e^{z_i}} \\\\=\n",
    "  (1-y_j)*(-t_j)$$\n",
    "\n",
    "For j != k:\n",
    "$$\\sum_{i=1, i \\neq j}^{K} \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial z_j} = \\\\\n",
    "\\sum_{i=1, i \\neq j}^{K} -t_k * \\frac{\\sum_{i=1}^{K} e^{z_i}}{e^{z_k}} * \\frac{-e^{z_j}*e^{z_k}}{(\\sum_{i=1}^{K} e^{z_i})^2}=\\sum_{i=1, i \\neq j}^{K} t_k * \\frac{e^{z_j}}{\\sum_{i=1}^{K} e^{z_i}} =\\\\\n",
    "y_j * \\sum_{i=1, i \\neq j}^{K} t_i$$\n",
    "\n",
    "Note by the dinifintion of softmax: $\\sum_{i=1, i \\neq j}^{K} t_i= \\sum_{i=1}^{K} t_i - t_j = 1 - t_j$\n",
    "\n",
    "Lastly, we sum up the 2 cases above:\n",
    "$\\frac{\\partial E}{\\partial z_j} = y_j(1-t_j) + t_j(y_j - 1) = y_j - t_j$\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q3: Top-Layer Error Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Your answer here.\n",
    "\n",
    "(a). Proved in Q1 regarding the logistic function:\n",
    "\n",
    "$dy/ dz = \\sigma(z)' = \\sigma(z) * (1 - \\sigma(z)) = y(1 - y)$\n",
    "\n",
    "We further apply the chain rule:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\partial E} {\\partial z} \n",
    "&= \\frac {\\partial E} {\\partial y} * \\frac {\\partial y} {\\partial z}\\\\\n",
    "&= (\\frac{(-t)}{y} + (1 - t) * \\frac{1}{1 - y}) * (y(1 - y))\\\\\n",
    "&= y - t\\\\\n",
    "&= \\frac{1}{1+e^{-z}} - t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(b).\n",
    "Since we know $y = \\sigma(z) = z$, so $\\frac {dy} {dz} = 1$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\partial E} {\\partial z} \n",
    "&= (2y - 2t) * 1\\\\\n",
    "&= 2z -2t\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Q4: Implementing Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Supplied Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Supplied functions\n",
    "\n",
    "def NSamples(x):\n",
    "    '''\n",
    "        n = NSamples(x)\n",
    "        \n",
    "        Returns the number of samples in a batch of inputs.\n",
    "        \n",
    "        Input:\n",
    "         x   is a 2D array\n",
    "        \n",
    "        Output:\n",
    "         n   is an integer\n",
    "    '''\n",
    "    return len(x)\n",
    "\n",
    "def Shuffle(inputs, targets):\n",
    "    '''\n",
    "        s_inputs, s_targets = Shuffle(inputs, targets)\n",
    "        \n",
    "        Randomly shuffles the dataset.\n",
    "        \n",
    "        Inputs:\n",
    "         inputs     array of inputs\n",
    "         targets    array of corresponding targets\n",
    "         \n",
    "        Outputs:\n",
    "         s_inputs   shuffled array of inputs\n",
    "         s_targets  corresponding shuffled array of targets\n",
    "    '''\n",
    "    data = list(zip(inputs,targets))\n",
    "    np.random.shuffle(data)\n",
    "    s_inputs, s_targets = zip(*data)\n",
    "    return np.array(s_inputs), np.array(s_targets)\n",
    "\n",
    "def Logistic(z):\n",
    "    '''\n",
    "        y = Logistic(z)\n",
    "\n",
    "        Applies the logistic function to each element in z.\n",
    "\n",
    "        Input:\n",
    "         z    is a scalar, list or array\n",
    "\n",
    "        Output:\n",
    "         y    is the same shape as z\n",
    "    '''\n",
    "    return 1. / (1 + np.exp(-z) )\n",
    "\n",
    "def Logistic_p(h):\n",
    "    '''\n",
    "        yp = Logistic_p(h)\n",
    "        \n",
    "        Returns the slope of the logistic function at z when h = Logistic(z).\n",
    "        Note the h is the input, NOT z.\n",
    "    '''\n",
    "    return h*(1.-h)\n",
    "\n",
    "def Identity(z):\n",
    "    '''\n",
    "        y = Identity(z)\n",
    "\n",
    "        Does nothing... simply returns z.\n",
    "\n",
    "        Input:\n",
    "         z    is a scalar, list or array\n",
    "\n",
    "        Output:\n",
    "         y    is the same shape as z\n",
    "    '''\n",
    "    return z\n",
    "\n",
    "def Identity_p(h):\n",
    "    '''\n",
    "        yp = Identity_p(h)\n",
    "        \n",
    "        Returns the slope of the identity function h.\n",
    "    '''\n",
    "    return np.ones_like(h)\n",
    "\n",
    "def OneHot(z):\n",
    "    '''\n",
    "        y = OneHot(z)\n",
    "\n",
    "        Applies the one-hot function to the vectors in z.\n",
    "        Example:\n",
    "          OneHot([[0.9, 0.1], [-0.5, 0.1]])\n",
    "          returns np.array([[1,0],[0,1]])\n",
    "\n",
    "        Input:\n",
    "         z    is a 2D array of samples\n",
    "\n",
    "        Output:\n",
    "         y    is an array the same shape as z\n",
    "    '''\n",
    "    y = []\n",
    "    # Locate the max of each row\n",
    "    for zz in z:\n",
    "        idx = np.argmax(zz)\n",
    "        b = np.zeros_like(zz)\n",
    "        b[idx] = 1.\n",
    "        y.append(b)\n",
    "    y = np.array(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    def __init__(self, n_nodes, act='logistic'):\n",
    "        '''\n",
    "            lyr = Layer(n_nodes, act='logistic')\n",
    "            \n",
    "            Creates a layer object.\n",
    "            \n",
    "            Inputs:\n",
    "             n_nodes  the number of nodes in the layer\n",
    "             act      specifies the activation function\n",
    "                      Use 'logistic' or 'identity'\n",
    "        '''\n",
    "        self.N = n_nodes  # number of nodes in this layer\n",
    "        self.h = []       # node activities\n",
    "        self.b = np.zeros(self.N)  # biases\n",
    "        \n",
    "        # Activation functions\n",
    "        self.sigma = Logistic\n",
    "        self.sigma_p = (lambda : Logistic_p(self.h))\n",
    "        if act=='identity':\n",
    "            self.sigma = Identity\n",
    "            self.sigma_p = (lambda : Identity_p(self.h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     2,
     65,
     81
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    def __init__(self, sizes, type='classifier'):\n",
    "        '''\n",
    "            net = Network(sizes, type='classifier')\n",
    "\n",
    "            Creates a Network and saves it in the variable 'net'.\n",
    "\n",
    "            Inputs:\n",
    "              sizes is a list of integers specifying the number\n",
    "                  of nodes in each layer\n",
    "                  eg. [5, 20, 3] will create a 3-layer network\n",
    "                      with 5 input, 20 hidden, and 3 output nodes\n",
    "              type can be either 'classifier' or 'regression', and\n",
    "                  sets the activation function on the output layer,\n",
    "                  as well as the loss function.\n",
    "                  'classifier': logistic, cross entropy\n",
    "                  'regression': linear, mean squared error\n",
    "        '''\n",
    "        self.n_layers = len(sizes)\n",
    "        self.lyr = []    # a list of Layers\n",
    "        self.W = []      # Weight matrices, indexed by the layer below it\n",
    "        \n",
    "        # Two common types of networks\n",
    "        # The member variable self.Loss refers to one of the implemented\n",
    "        # loss functions: MSE, or CrossEntropy.\n",
    "        # Call it using self.Loss(t)\n",
    "        if type=='classifier':\n",
    "            self.classifier = True\n",
    "            self.Loss = self.CrossEntropy\n",
    "            activation = 'logistic'\n",
    "        else:\n",
    "            self.classifier = False\n",
    "            self.Loss = self.MSE\n",
    "            activation = 'identity'\n",
    "\n",
    "        # Create and add Layers (using logistic for hidden layers)\n",
    "        for n in sizes[:-1]:\n",
    "            self.lyr.append( Layer(n) )\n",
    "   \n",
    "        # For the top layer, we use the appropriate activtaion function\n",
    "        self.lyr.append( Layer(sizes[-1], act=activation) )\n",
    "    \n",
    "        # Randomly initialize weight matrices\n",
    "        for idx in range(self.n_layers-1):\n",
    "            m = self.lyr[idx].N\n",
    "            n = self.lyr[idx+1].N\n",
    "            temp = np.random.normal(size=[m,n])/np.sqrt(m)\n",
    "            self.W.append(temp)\n",
    "\n",
    "\n",
    "    def FeedForward(self, x):\n",
    "        '''\n",
    "            y = net.FeedForward(x)\n",
    "\n",
    "            Runs the network forward, starting with x as input.\n",
    "            Returns the activity of the output layer.\n",
    "        '''\n",
    "        x = np.asarray(x)  # Convert input to array, in case it's not\n",
    "        self.lyr[0].h = x # Define first activity is just input\n",
    "        for i in range(self.n_layers - 1):\n",
    "            next_input_cur = np.asarray(np.asarray((self.lyr[i]).h) @ (self.W[i]) + np.asarray(self.lyr[i+1].b))\n",
    "                    #Input Current for next Layer\n",
    "            self.lyr[i+1].h = np.array(self.lyr[i+1].sigma(next_input_cur)); \n",
    "                    # apply activation function to input current\n",
    "        \n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        \n",
    "        return self.lyr[-1].h\n",
    "\n",
    "    \n",
    "    def Evaluate(self, inputs, targets):\n",
    "        '''\n",
    "            E = net.Evaluate(data)\n",
    "\n",
    "            Computes the average loss over the supplied dataset.\n",
    "\n",
    "            Inputs\n",
    "             inputs  is an array of inputs\n",
    "             targets is a list of corresponding targets\n",
    "\n",
    "            Outputs\n",
    "             E is a scalar, the average loss\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        return self.Loss(targets)\n",
    "\n",
    "    def ClassificationAccuracy(self, inputs, targets):\n",
    "        '''\n",
    "            a = net.ClassificationAccuracy(data)\n",
    "            \n",
    "            Returns the fraction (between 0 and 1) of correct one-hot classifications\n",
    "            in the dataset.\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        yb = OneHot(y)\n",
    "        n_incorrect = np.sum(yb!=targets) / 2.\n",
    "        return 1. - float(n_incorrect) / NSamples(inputs)\n",
    "\n",
    "    \n",
    "    def CrossEntropy(self, t):\n",
    "        '''\n",
    "            E = net.CrossEntropy(t)\n",
    "\n",
    "            Evaluates the mean cross entropy loss between t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs:\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs:\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        last_layer_h = self.lyr[-1].h\n",
    "        E = -np.sum(t * np.log(last_layer_h) + (1 - t) * np.log(1 - last_layer_h))/len(t);\n",
    "        \n",
    "        return E\n",
    "\n",
    "    \n",
    "    def MSE(self, t):\n",
    "        '''\n",
    "            E = net.MSE(t)\n",
    "\n",
    "            Evaluates the MSE loss function using t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs:\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs:\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        last_layer_h = self.lyr[-1].h;\n",
    "        return np.sum((last_layer_h - t)**2)/len(t);\n",
    "        \n",
    "        \n",
    "        return 0.\n",
    "\n",
    "    \n",
    "    def BackProp(self, t, lrate=0.05):\n",
    "        '''\n",
    "            net.BackProp(targets, lrate=0.05)\n",
    "            \n",
    "            Given the current network state and targets t, updates the connection\n",
    "            weights and biases using the backpropagation algorithm.\n",
    "            \n",
    "            Inputs:\n",
    "             t      an array of targets (number of samples must match the\n",
    "                    network's output)\n",
    "             lrate  learning rate\n",
    "        '''\n",
    "        t = np.array(t)  # convert t to an array, in case it's not\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        dEdz = 2 * (self.lyr[-1].h - t)/ len(t);\n",
    "        #print(np.shape(dEdz));\n",
    "        for i in range(self.n_layers - 2, -1, -1):\n",
    "            back = self.lyr[i];\n",
    "            dEdb = np.sum(dEdz,axis = 0)\n",
    "            dEdw = (back.h.T) @ dEdz;\n",
    "            #print(np.shape(dEdz))\n",
    "            #dEdz = back.sigma_p() @ ((self.W[i].T) * dEdz);\n",
    "            dEdz = dEdz @ (self.W[i].T) * back.sigma_p();\n",
    "            self.W[i] = self.W[i] - lrate * dEdw;\n",
    "            # E(y,t) = E(sigma*(M*layer[i-1].h + b), t)\n",
    "            self.lyr[i+1].b -= lrate*dEdb\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def Learn(self, inputs, targets, lrate=0.05, epochs=1):\n",
    "        '''\n",
    "            Network.Learn(inputs, targets, lrate=0.05, epochs=1)\n",
    "\n",
    "            Run through the dataset 'epochs' number of times, incrementing the\n",
    "            network weights for each training sample. For each epoch, it\n",
    "            shuffles the order of the samples.\n",
    "\n",
    "            Inputs:\n",
    "              inputs  is an array of input samples\n",
    "              targets is a corresponding array of targets\n",
    "              lrate   is the learning rate (try 0.001 to 0.5)\n",
    "              epochs  is the number of times to go through the training data\n",
    "        '''\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        for i in range(epochs):\n",
    "            shuffled_input, shuffled_target = Shuffle(inputs, targets)\n",
    "            self.FeedForward(shuffled_input);\n",
    "            self.BackProp(shuffled_target);\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 5 Classes in 8-Dimensional Space\n",
    "np.random.seed(15)\n",
    "noise = 0.1\n",
    "InputClasses = np.array([[1,0,1,0,0,1,1,0],\n",
    "                         [0,1,0,1,0,1,0,1],\n",
    "                         [0,1,1,0,1,0,0,1],\n",
    "                         [1,0,0,0,1,0,1,1],\n",
    "                         [1,0,0,1,0,1,0,1]], dtype=float)\n",
    "OutputClasses = np.array([[1,0,0,0,0],\n",
    "                          [0,1,0,0,0],\n",
    "                          [0,0,1,0,0],\n",
    "                          [0,0,0,1,0],\n",
    "                          [0,0,0,0,1]], dtype=float)\n",
    "n_input = np.shape(InputClasses)[1]\n",
    "n_output = np.shape(OutputClasses)[1]\n",
    "n_classes = np.shape(InputClasses)[0]\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 100\n",
    "training_output = []\n",
    "training_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    training_input.append(x)\n",
    "    training_output.append(t)\n",
    "\n",
    "# Create a test dataset\n",
    "n_samples = 100\n",
    "test_output = []\n",
    "test_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    test_input.append(x)\n",
    "    test_output.append(t)\n",
    "\n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a Network\n",
    "net = Network([n_input, 18, n_output], type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 3.6170513253334455\n",
      "     Accuracy = 26.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=500, lrate=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "Cross Entropy = 0.43837158388429015\n",
      "     Accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Training Set')\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set\n",
      "Cross Entropy = 0.47097322385259566\n",
      "     Accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Test Set')\n",
    "CE = net.Evaluate(test[0], test[1])\n",
    "accuracy = net.ClassificationAccuracy(test[0], test[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## You can also try using the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " #import Network\n",
    " #importlib.reload(Network)\n",
    " #net2 = Network.Network([n_input, 18, n_output], type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# net2.Learn(train[0], train[1], epochs=500, lrate=1.)\n",
    "# print('Training Set')\n",
    "# CE = net2.Evaluate(train[0], train[1])\n",
    "# accuracy = net2.ClassificationAccuracy(train[0], train[1])\n",
    "# print('Cross Entropy = '+str(CE))\n",
    "# print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "# print('Test Set')\n",
    "# CE = net2.Evaluate(test[0], test[1])\n",
    "# accuracy = net2.ClassificationAccuracy(test[0], test[1])\n",
    "# print('Cross Entropy = '+str(CE))\n",
    "# print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1D -> 1D (linear mapping)\n",
    "np.random.seed(846)\n",
    "n_input = 1\n",
    "n_output = 1\n",
    "slope = np.random.rand() - 0.5\n",
    "intercept = np.random.rand()*2. - 1.\n",
    "\n",
    "def myfunc(x):\n",
    "    return slope*x+intercept\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 200\n",
    "training_output = []\n",
    "training_input = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    training_input.append(np.array([x]))\n",
    "    training_output.append(np.array([t]))\n",
    "\n",
    "# Create a testing dataset\n",
    "n_samples = 50\n",
    "test_input = []\n",
    "test_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx] + np.random.normal(scale=0.1)\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    test_input.append(np.array([x]))\n",
    "    test_output.append(np.array([t]))\n",
    "\n",
    "# Create a perfect dataset\n",
    "n_samples = 100\n",
    "perfect_input = []\n",
    "perfect_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x)\n",
    "    perfect_input.append(np.array([x]))\n",
    "    perfect_output.append(np.array([t]))\n",
    "    \n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]\n",
    "perfect = [np.array(perfect_input), np.array(perfect_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = Network([1, 10, 1], type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.35753196157541495\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE = 0.009153094638339992\n"
     ]
    }
   ],
   "source": [
    "# On training dataset\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('Training MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE = 0.011992262820889135\n"
     ]
    }
   ],
   "source": [
    "# On test dataset\n",
    "mse = net.Evaluate(test[0], test[1])\n",
    "print('Test MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate our model and the TRUE solution (since we know it)\n",
    "s = np.linspace(-1, 1, 200)\n",
    "y = net.FeedForward(np.array([s]).T)\n",
    "p = [myfunc(x) for x in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEKCAYAAADuEgmxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJztnXucFNWVx39nXjCjCNKgQaB7NJqoia6PWRPjJkaTbNDdhTyM0YwENyIyJtG8NkFnjcbIqhiT9RUV8IEM8ZHE3eBG1iiKMS9wfOAbQYUBZHkMgsDwmOk++0dVQ01PVfWt7qrq7pnf9/O5n+muunXr1J2Ze+rec885oqoghBBCTKgqtQCEEEIqByoNQgghxlBpEEIIMYZKgxBCiDFUGoQQQoyh0iCEEGIMlQYhhBBjqDQIIYQYQ6VBCCHEmJpSCxA2I0aM0MbGxlKLQQghFcVzzz23SVVH5qvX75RGY2Mj2tvbSy0GIYRUFCKyyqQel6cIIYQYU1KlISLjRGSZiKwQkWk+9c4SERWRpjjlI4QQ0puSKQ0RqQZwG4AzABwN4FwROdql3hAAlwBYHK+EhBBCcinlTOMkACtU9W1V3QPgAQATXOr9FMAMALviFI4QQkhfSqk0RgNY7fi+xj62FxE5HsBYVf2fOAUjhBDiTimVhrgc25sRSkSqAPwCwPfzNiQyRUTaRaR948aNIYpY/sybBzQ2AlVV1s9580otESGkP1NKpbEGwFjH9zEA3nV8HwLgowAWichKAB8HMN/NGK6qM1W1SVWbRo7Mu8243zBvHjBlCrBqFaBq/ZwyhYqDEBIdpVQazwI4QkQOFZE6AOcAmJ89qapbVXWEqjaqaiOAvwEYr6p0wrBpbQW6unof6+qyjhNCSBSUTGmoag+AbwF4DMDrAB5S1VdF5GoRGV8quUwph2Whjo5gxwkhpFhK6hGuqo8CeDTn2I896n46DplMyC4LZd/ys8tCANDcHJ8cyaR1b7fjhBASBfQID0B2dnHeee7LQpdeGq8806cDDQ29jzU0WMcJISQKqDTykFUUIsDEie5v9lk6O+NdpmpuBmbOBFIpS75Uyvoe52yHEDKwEFXNX6uCaGpq0rACFuYuQ5mQSgErV4Zye0IIiQ0ReU5V84Zq4kzDB7fdSfkIYoQuB2M6IYQEod+FRg+TQnYhmRqhy8WYTgghQeBMw4egu5CCGKHpY0EIqUSoNHxw250kdvCTVApoaSncCE0fC0JIJcLlKR+yCqC11RrMk0lLkYSxfEQfC0JIJcKZRh6am63dUJmM9TMsewN9LAghlQiVhoM4djNl7zFxIlBfDyQS9LEghFQOXJ6yiWM3U+49Ojut2cXcuVQWhJDKgM59NiNGWIN4LmE66zU2utsx6BBICCk1dO4LwLx57goDCHc3E3dMEUIqHSoN+PtGhLmbyast7pgihFQKVBrwf9MPczcTd0wRQiodKg34v+m3toa3i4pRaQkhlQ6VBtxnAFnCzrsdld8HIYTEAZUGes8A3GBMKEIIsaDSsMnOALKxpXJZtYohzAkhhErDwZ70Howek/Y8rxr+chUhhFQSVBoO5rw4B+s/PhmD6zO+9bq6rDzhnHUQQgYaVBoOPt34aVzWksTsWYJEAgDULu5EOetgVj9CSDlCpeHgiMQR+MlpPwEg2LlTAYhdvInCSJ6NUbVqFZfECCHlBWNPueAVI8oLEWsLbVgwRhUhJG4Ye6oIvD3E3RVs2GFAGKOKEFKulFRpiMg4EVkmIitEZJrL+e+JyGsi8pKILBQRD0+KcPFSAsOGZwKFASnULlFMjCraQgghkaKqJSkAqgG8BeAwAHUAlgI4OqfOaQAa7M8tAB7M1+6JJ56oBbNpk+rChdo2N60NDaqWRcEqDQ2qbW1WSSYzCsnofiM3alubdemu7l29mmprU8828lHotcXckxAysAHQriZjt0mlKAqAkwE85vh+GYDLfOofD+DP+dotSmncfLPVJWPHatv4BzV1yB4VUU2l3AfennSPqqqu6FyhI2aM0P9d/r97zyUSvQfvbEmlzERpa7Pq+t0/l1SquHsSQgYupkqjlJn7RgNY7fi+BsDHfOpfAGBBpBJNngyMHAnMmYPm/zkXzZkMcPLJwFNPAYMG9aleXVW99/Npjafh2IOPBQDcOnszOjsPhNvOK1O7RHNz8LhUtIUQQqKmlErDbS+rq6VZRM4D0ATgVI/zUwBMAYBkMVbp+nrgnHOssm6dZRBYvnyfwrj+euDoo4Fx44Da2r2XfXD4B/HQVx7a+/2Hl/XAa6tulLkzkkn3XVfM10EICYtSGsLXABjr+D4GwLu5lUTkswBaAYxX1d1uDanqTFVtUtWmkSNHhiPdqFHAD34A3Hmn9X3XLuDmm4Hx44HRo4HvfAd44QVrBSiHXZ3eMkSZO4P5OgghUVNKpfEsgCNE5FARqQNwDoD5zgoicjyAO2EpjA0lkHEfgwdbThLz5wOf+hRw++3ACScAv/xln6rJpLdDYJj5OXKptHwd3OlFSAViYviIqgA4E8CbsHZRtdrHroalJADgCQDrAbxol/n52izKEB6Ezk7V225TXb3a+v6736mee662XfayJoZnXA3S2VI7aI/ec9/ueOT0oRBje5j35k4vQsoHGBrC6REeFnfeiXnffRZTdt6ELuyXt3oyqVi1yj9ESZRkQ5V0de071tAQ38yEXu+ElBf0CI+biy5C64iZRgoDAFavFqQzaZw25zS0vdQWsXB9aW3trTAA9zhaUS0hcacXIZUJlUaIdKwx785kEti8czNqqmpQV10HAOhOd2Nn985IZMsd/L1iazkHba/AiRdfXLwiKcbrnRBSQkzWsCqpxGbTcMHLuU6kt42joWqntn37r6p79qiqaiaTUVXVO9vv1A/87APasaUjVLnc7Aci7rI6HQG9n6d4WwRtGoSUFzC0aXCmESJeW16nThV7R5MiNWwrZh74QzTfcjJg216ylo11fzkNO2a8itSBY9DYCEy/bSV29ewqWi63pSjVvqltc7fnei0V5ZrBnMtapstZlbbTixBiY6JZKqmUcqahargjqadHdeFCVXuGoRdfrG3H3aANg7pz3uozWjdka9Fv3347ufxk9ZppeJVEQrWujrMHQioRcPdUBXHddWhsbcaqzFjX0w0NwC9u3Y70R+fiG8d/A4Nq+oY08WLePGDiRFcfxLw7ldx2WIm4t+UHd0QRUv5w91QlMW0aOnSM5+muLuCyyxUXP3ox3ux8M1DTra3ug7xIfk9xtyWkqVP7LsHlgzuiCOk/cKZRJuTLFihQvPj2Yhx76McBAP/xzH/goP0OwuQTJvu2W1XlPTMo9Fc/b56ljEyzG3KmQUj5w5lGmZHPQOxmRHeSxCoce/w44HvfQ2bFcjzx9hP46+q/7j2fUfd8s15bWFNFpLNqbraUgEkbjH1FSD/DxPBRSaXUhnA3TLeXtrW55+FoaMho25XLVM85R7WmRnXkSM3s3q1de7pUVXV553I9/ObD9S8dfyn43mE9V22t9QylCE1CCCkclHsSpqhKKZRGvh1TfruQsvWdbSQSPgPv2rWqjz9ufU6nVc84Q5//+b/pp2adou++/66qqm7csVH39Owxli/KZy8XKkVOQkoFlUZMmLzJeznSOd/OC9qq+u67qh/7mHXB/vurfutbqm+8oV984It6wp0n7HUaDOs5K3XQpSMhIfmh0ogJkxSrQf0d3NpQ9Rm4lyxRnThxr+Z55OHr9K7n79p73YLlC7Q73V3wM1b6oMs0uITkh0ojJrxmESL76rgNuiYlXxt9Bu7161VvvNFyHlRVnTVLl9x+heIq6O3P3l7wM5oOuuU6GzH5HREy0KHSiImgA2qhM42C3pY/+1nNAPrfJw3VHVe1qm7YoIveWaRzl87VnnSP8TMWqhidSq2UCoUzDULyQ6URE0GXbkxnHaZ2kdyBu/fAnLGM5meeaVUeNEgnXnuSNv5nYy9DeT6KWYLLKgivgIl+CiQsRVPpy2uExAGVRowEHdzyzTqqq3u30dZmHfMbuPMOjK+/rnrRRZp+epGufG+l6jvvaM9jC/SLD3xRFyxfkFfeQo392T4JoiBN7xmEcl06I6RcoNKoAEwGRr+ZibOu18BcXe0xUP7wh9pxAPQj3x2kD94yVXXnTu1Od2s6k957X+cg29JS2Lbi7DVBluLytUcICR8qjQqhUB+P3NmIycDcSyHt2qV6zz2aPvYYTQtUDzpI75j+Jf3IbR/R2+/aGvgt308Bmthyco3SNF4TEi9UGv0EP2XgVDZunuRGb+qZjOoTT6iOG6ePfPNzOvHhiZpMZsyuzcFLAZrYcTjTIKS0mCoNBiwsc7wCGeaGKK+ttY7t2ePfngiQcQ9TBfT0ADU1qKpSqEqf077X5sEZ5DBX9oaGvgmY3MKyu9UjhIQDAxb2E9wCGbrltOjuBoYMyR9E0C8H97wHa9DY2LftLCM+0IVCXzKyQQ5Vgblz82fsY2Y/QsoTzjQqgOxbekeHNeh7hSTPzgT8Zidz57oPvG5v9k5qq3ag+wsX4rU7rsBRI48q+FkIIeUJZxr9hFyFMX2692xC1T8vh6r3m7pbHvEsqbEZ3PXjtXjyZxfiqMSHgQkTcNPPz8bvX/2vwM9DCKlsqDTKmOzb/6pV1oC/apX1/cwzvXNvZG0GbvgtXXll1xMBVnZUYeKVH8Jph54GbNyInnVrMfOtX+Phn34NuPZaYOtWV9n98ocQQiqTkioNERknIstEZIWITHM5P0hEHrTPLxaRxvilLB1ub/9dXcCjj+5b73dDta/iyJcMycvW0ef4wQejZvGzePHLj+PG7acAl1+OFceOwSm3HIdXNrwCwFvZRaE4qJwIiRmTLVZRFADVAN4CcBiAOgBLARydU+diAHfYn88B8GC+dvvTllsTXwXTLbkmXupu22ITiTzXPv+8Lrpkgh51y5FWPo+FC3XsIXti2S7L8CCEhAfK3U8DwMkAHnN8vwzAZTl1HgNwsv25BsAm2MZ7r1IqpRFFmIpiYz4FlTuRUN1vv75tmQzEmUzGiq47dqwC6Vgc8+jLQUh4mCqNUi5PjQaw2vF9jX3MtY6q9gDYCiARi3QBiGo5xm27bW0tsH37vuUYN/uGaV7uXLk7O92N4V1d1lKZHyICVFdD//hHDB+ywbWOarhLSF52GK/jhJDiKaXScDPX5u7/NakDEZkiIu0i0r5x48ZQhAuCl+0h30Cbj1xfhUTC+tnZuU85zZkDTJpUmD+Dm9zqsQPbdCCWxkbcfPsH0FDv7gUYpn3D2A5DCAmNUiqNNQDGOr6PAfCuVx0RqQEwFMDm3IZUdaaqNqlq08iRIyMS15so33izTnGZDLD//n09vrOG8WydlSvNHeCCyBdkIG5uBmbOqrIN9X21UFcX0Hp58f5BbjMx01lWWNAQTwYapVQazwI4QkQOFZE6WIbu+Tl15gOYZH8+C8CT9tpbWRHXG2/YyslLvqA7r9zIKjvx2P/b0aHAI494T20M7zFzpjUDy1JfX3BzgYlzlxgh5ULJlIZto/gWLGP36wAeUtVXReRqERlvV7sLQEJEVgD4HoA+23LLgbjeeMNWTl5yT52af7nL9A3bU+bqd5GeMB4X/Pg4LFm7pLAHsNm5c9/nzs74Bu6oliUJKWtMrOWVVPrT7im3e4S9xbQQuYPI4Vl3Trcum3WdHnzdCH3olYdUX31VMw8+qJpOB5I/zh1UuX3ltdWZ4dtJJYJy33IbVelPfhpuRKWcgrQbdKD2a3vHnh1W4qdLLtE7ToR+bup+urXtLmv7rgFx5d3wS1nLLb+kP0ClQYwJOoOJZKDu6dG777hIJ1w4RDOA6pFH6oZfzS44SVXYA7fXfXL7opCZH1PRknKASoMYE3TgNa1f0GCYTqs+9JBu/7ujdciXztfaQbv73MfppR5U4eU6MyYSZvKF5XnvJg+92kk5QKVBjAk6cyg0t3mQwXDHrm067OD3PAdqZ1umyilfBkE/+aKa0dCrnZQLVBrEmEIGrrCXjdzay5f3PGj6WZOUuH7yRTEjYC50Ui5QaRBjohgQgwyGXvfPN8iLZAI9k0nxG6xLFV+MkDgwVRrMp0EiSa1q6lMyb54VBsXN32HzZqCuzvse+43otPLc3ntvH1d5v6RShcgN9PbOD+J57+fTUg5e7YQEwkSzVFLhTMOfuHbqFGr3yC21te6Rd0UyCmQ0NWK73rjfufql8+v1nduvVd2zR1XzL20FtWlE3Q/cPUVKDbg8RXKJe6dOoXYPt6WabFvZJSTn+UF1u3Xov5yn6/aH6qGHavddszSVzLi2lUgUtnuqULj8RCoFU6UhVt3+Q1NTk7a3t5dajLLEK394IgFs2hS7OKiqsobQfIhYS0KA9zMkk4pVty8ArrwSXz5+OTYPuQFL7riw1xJVQ0Pxy25Z3HK3u7Xr9YzOZyKkHBCR51S1KV892jQGEF6BDTs7zWI1hR3R1TRmlrOe1zOsXi3AmWdCFy/GR8dPxj9/+X3M/MV2pOrWQaBIJTVUhWEaqJDh20m/w2Q6UkmFy1Pe+C0HmWxfjSLuVT6bRu498i339FoSG7Vb//2oS1WuhD5y+hjVBx4IHNvKjSBLTnTeI5UCaNMgubS1eQ/O+fwColqbz7V7tLT420H8BmG3c/X1Gf3SpJt0xzFHqgLa/g8f1NdXthclcyHOkEHztdMwTuKGSoO44uX7kG/wLycnNK9B1Vex9fSo3n+/nto6Wg+/+XArSOLzz6tmvH09vIjSuM2ZCSkVoSoNAKeYHCuHQqXhT6GDUiXsAvLbZpt9vg3bN+jiNYtV33pLe2qq9N+aD9LlD90RSHm0tBQWqNBkBlEJ/Uz6J2ErjedNjpVDodLIT9Q5NKKimC282UF+73Xd3frCnT/RhlbRXx8N1aYm1d//Pq/y8AqR3tKSX3aT/iunGR0ZWISiNACcDOD7AFbDypyXLVcBWGpyg7gLlUZ0eA3apUpAlasITEOHOAfr9e+t0fTsWaqNjTrr76v1/Hlna9eeLk85Cp0JmF7HmQYpFaZKI9+W2zoA+wOoATDEUd6HlbObDCDcwmjElSfbLSyI9V6z756A5YeRD2dK1oOGjUbVBZOBZcvQecmF6OjZhME1g4HLLkPXosf7bDN28xEB8udpN83vzrAipNwxcu4TkZSqevy7lBd07osXr4E0lbIUS1iYOAJm7+k3uGfxcq5TVcjGjdh+4jEYc+xn0PXYXehO1/e6zk2OfM8bpJ9MHQcJCZOwnfvuFZEnc0uRMpJ+gOkbdJZCHQRNnOGy93R7WzdtT0SAgw5C99Lnkfnzz3spDMBSGCK9rzGZCQSZQRQaGJGQOKgxrPcDx+fBAL4MoCd8cUilkUx6hfXoeyy7lJVdZnIuK+UbGKdP732tlyzOtlpbrXvkzg5MBvkDh4/G9vfdz6kqUikJNBPInr/0UssDHwDq673rE1K2mBg+3AqApwu9NspCQ3i8BNlVVayR1y9ooWmK1yCGek95a9fqgy2f0o6lfzRryCGHn2MiHfpIKUHIW26HO8oIAJ8HsMzk2rgLlUb8mA54YW4njXKQ9VVO9RmdfeYcHXIZ9MLxonrhhaqrVhm166WEEonSb2cmJGyl8Q6At+2fywH8AcA/mFwbd6HSKF8qYTupydZeVdWVy5bo2kvOV62t1TcPrtVv3X2Wbti+wbftoDk+ougXzmiIF6EqjUoqVBrlSzk4COYjsGJbtUrvbv0nHXrtUP2/bf+n+vTTmlm/PlDbfg6JTood8Cuh/0npCHumMRiWU9/DAH4L4LsABptc69HecACP27OWxwEc6FLnOAB/BfAqgJcAfNWkbSqN8sZk4CvF27BzScrvzd9Lpq27tmrbnG5NVXUokNahQ9dp28ztfe4RJBe6U1GFMeBXwkyPlI6wlcZDAO4CcJpdZgL4tcm1Hu3NADDN/jwNwPUudT4E4Aj78yEA1gEYlq9tKo3KphRvwyae5PkM765yY4e2nLREx4zp9o3ia/LMYQz4DFFC/AhbafQJGeJ2zLQAWAZglP15lIlRHcDSrBLxK1QalU0xg2OQGYqzbnV1MIXhJpP3LCVtpACdM52sPM5nCGPA50yD+BG20rgXwMcd3z8G4Jcm13q0tyXn+3t56p8E4HUAVR7npwBoB9CeTCZD70wSH4UOjkFmKKYxqrIDqonNIYiRe8yY7sDPkG8Jy3TZjzYN4kXYSuN1ABkAK+2SsW0NLwN4yeOaJwC84lImBFEa2ZmIU2n5Fc40KpuoAwL61fW61qTtYEbutGZmzrRsIKl9A72XYkgkVGtr+x6vqzNf3srC3VPEi7CVRsqvmLSR057R8hSAAwA8D+Arpm1TaVQ2hb4NB5mhmMwKnPc0kclvq25uOXjwGm3DudogOwIoGndlosplJxIOYSuNuSbHTAuAG3IM4TNc6tQBWAjgO0HaptKofAp5Gw5jplFd7Z9mNuiur5YWL2WT0dTI4hSGUyHSwE3CIGyl8XzO9xoAr5lc69FewlYIy+2fw+3jTQBm25/PA9AN4EVHOS5f21QaA5NibRpRre17KRvv2U6mj1z57BmcaZAwCEVpALgMwDZYwQnftz9vA9AJ4FqTG8RdqDQGLoXunirF2r7nbKdugybr3lWRjCaTGSObRX8ycJf69zKQCXumUZYKwq1QaZBKwH2gz+i9FzyumkzqtjroB68eoW1L2/bW9xtMc3dYJRKVN+D2J+VXiZgqDdN8GgtE5FO5xfBaQkgOzc1WlsFUygrdnkoBM2cKJs3+LPDmm3j/hmvwd40fwweHfxBYsgT/0vgkXnxjy94cG0DvvCR//jOwc+e+9js792VQLDSHiZMw2siHW3ZGZ5ZFUiaYaBYAjzjK4wC2AnjS5Nq4C2caJG4iX1IZN04vPx06/N9rdfPipwPt1Aojgm5cMwAa9EsLogxYCGAsgPsLuTbqQqVB4iSWAXXbNn3hmm/qjacNVgU01bCh6J1XXkZyNwUYl6GdBv3SErXSEAAvF3Jt1IVKgwTFOVAmElYxnTXEOtC9957qFVeo5IQmKWa7rhMvBRikjWKgTaO0mCoNI5uGiNwiIjfb5VYAf4IVC4qQiiabgnbVKmuY6uy0iuq+dLR+6/dBc6QXxbBhwNVXIzlGXU+L9D7e0AAkEu5NuaXj9bIpVFebt2GKm43E3c7DHOnlhqkh/DUAb8Ly5P4bgB+q6nmRSUVITLgNlE7yGWK9Bs5iBtR8TL+uGg0NOQdrduDrR/4BqUO6ew24Z59tDcBOGhqAM8/sO2h7Kbp0Gn3u59WGCbmK2qmcm5stQ3/W4E+FUYb4TUNgOfHNALAJVjiPF+zPMwDUmkxl4i5cniJBMAkp4rcMU6ollV62hzHdesu4maqDBqnW1Ojkn/693rbkNk+D+Wc+Ezyvh7m3e37ZC1nSo/9G9CAk575fAJgNYIjj2AGw8mncZHKDuAuVBgmCSaDBfPaJshnQ1qzR3d9q0XE/P1F/sugnmhrrbvvwCgUfZKeVX785+8Ctb4LukqKtIx7CUhrLAYjL8WoAy01uEHeh0iBByBcm3S0/eFj3jVLRpDNplZyQJCYzKlO58s3QGhq8ZyMmmQqdcFdVPISlNN4s5FwpC5UGCYrb7imnwgj77TauN+fU6G6PmYa7MgkyCJvM0ILMaPyUM/034sFUaeQzhL8mIl/PPSgi5wF4o0AzCiFlhdP4ummTVVIpa2hyEsQ72c+DOi7P5+nX1/QxYEtNF0Z88mFXw/b06QHant7XOJ5LOu1+fPPmfbukAMtQn+1rtx1rpdhsQHzw0ygARgNYDGARgBsB/AzA0wCWABhtopXiLpxpkDAo5u0230wizjfnXstNh+zR1iue0QXLF2jbjes0ecBmhWR0bDJdUMImP8c/v2KSvMpZhzaNeEDIAQtPB/BtAJcA+IzJNaUqVBokDIpZR893bVms0f/yl/ro4VBcBX34yrMtx0EHUaXPzW3DVIGWzWaDfkyoSqOSCpUGCYNi3m7zDYTl8uacWbpUn5l4qqYFqsOG6a+mn6u3LblN05m0r2JzG8BNZh1u3vZloUCJqlJpEFI0hb7dmi65mLTtN0AHlcvzuhdeUB0/Xr867XD9xF2f0EwmoyLeO6/8Ngj47apyU5TF+HuQcKHSIKREhDWTcGuntla1ri54234y7VMmGR0ztkfbrnhdx1atKshOEdTG4TVzKTV+MpWjvGFApUFICQljYAkyAOdbzvFqy9Whb3Bav3bYw1pVvT1nhuE/+8g+t6l9w3mdKYXkag9DWTsVbH+dGVFpEFLhmIQ4MR18g7S1dwZwzduaql+vgrQeUL9Kc/OXmyy9eflqmCo7JyYDdhiDut/yYn+2wZgqDbHq9h+ampq0vb291GIQUjSNjZbfggmp1L6MfsW2BVi+E5mM/eWll/CHpQ/j7H/7DrauH+Zad+5c9+CCVVXWsOpGQ0OwKLZez+B8dpM6+fCSORv40evc3v6qUETkOVVtylfPNMotISRm3BzoamuBurrex0wc89zaMg6dfuyx+MeJV+G2G4ehYVBvjz0RxdSp3gO/lwNedXXwsOcmYejDCFXv50xIR0MqDULKFrf8EvfcA9x9d/CcE165Km66yV0xbd/e15u9uRmYeVc1kqP2QJDB6KqVmFvzDXx3vwtw35JZmNuW7uMF76Ws5swJHvbcZMAOY1D3knn6dP9zAwaTNaxKKrRpEBKM3Nhbxruz3nhDdeJEvfzshNaeNUnr6zOu14W12ygum0Zun3D3FA3hhPQbwh7ACjH0prt26Kgxu2MxEMexe2qgYqo0aAgnpELJZsBzBj8MalzOxc8I7Gfo9b5O0ZNWVEnVXplbWy0bQzJpLeswO195UNaGcBEZLiKPi8hy++eBPnUPEJG1dm5yQoiNV7TcSy8tLA0rULhNwOv8QdUdOOamD2PZpmW+aV5J5VAqQ/g0AAtV9QgAC+3vXvwUVmRdQogDrx1BnZ2FD8yFGnpdr6vP4IKvPIVRw1NIDk2i9ZJtkYaE9wtHT0LEZA0r7AJgGYBR9udRAJZ51DsRwAMAzgdwq0nbtGmQgUKYHuNOQo9tpaq6a5cK3NPPhhESvj97ascFytkQDmBLzvd2fWDZAAAU80lEQVT3XOpUwcrjMTaf0gAwBUA7gPZkMhlyVxJSngQJ2RF3ljs3BZIc2+MqWzKZKfp+leSpXa6GelOlEdnylIg8ISKvuJQJhk1cDOBRVV2dr6KqzlTVJlVtGjlyZHGCE1IhuPleGDnrRYyX7eKf/rm6b7a/2h346iVLMW/mdjSmdO/S0sUXB1tqCsOpLw76hV3HRLOEXWCwPAVgHoAOACsBbALwPoDr8rXN5SkykHGbfYhYIcjjwjQXRzKZ0UtnLNa5bWltqN7pO1PKt9QUNP+HE2dej2ysrKh8M8p5RoQyX566AcA0+/M0ADPy1D8ftGkQYkRLi3/Oi6gJms7W1DbjN7B62TTy5evwW+KLIrJtnKl+g1LuSiMBa9fUcvvncPt4E4DZLvWpNMiAI8okUFES9P6mEXjzDaxu/eUlS3W1dT6RyK+owuzPUv9u/ChrpRFlodIgpSTOkBlehPk2W8jzBJXddKaRPGRPYPmChoR367Ow+7Ncd3lRaRASM2EOCMW8kRZyrVda2UKfJ0i4j+wA7Dd411Rt18NPPVd3f+2rVnpaNctGWIzCiGKmYdo3pYBKg5CYCXNwKebt1nSw9xu0Gxq8l27CWErxMthn229p6T2wTr38Uf3+5Seq7r+/KqBvTD1Lk0n3pFBu2QgLKQMhW58TKg1CYibMZYxiFZDJjqFCB1bn88Rud3nvPV09/Udad1WNwif9bBgzjIEQ2dYJlQYhMRPmTCPqt9tilm6yz1Mqu8uu7l16+7O36+gx3aEri/44gzDFVGkwCRMhIRFmgh6vpElhRYQ1cXpLJPyfxytgokksqSCBEXNjSv3mwUGY2jQV119Xg4YG7S0fupCo3+H5PKmU9TmbujX3fJh93G8x0SyVVDjTIKWkUpYx8s00TBIoxWV3yednMTaZVpGMpg7eqW0n3qhf/MQ3dVB9j+81TntNIlG+v6c4AZenCCFe5DNEmwyiUdtdCrnH5q7NOupno/TLrb/V1JgeFaQ1NWidtn3/edV0esAYtQuBSoMQ4kuxs6I4BuBCZjNde7p0x54dqu+/r4v+4yL9zIWDtOMAqH74w5oa/n5odqeglPss1FRp0KZByACluRlYudLKyLdyZfC1/KjtLkBhSaHqa+vRUNsADBmCTRM+h80nHIURv7wXGDIEHZv3c70m6sCG/SJQoQ3TvRJCypZiU9rOmwdcfrli9WrB2LGKtZ1bkN7RN1FoKmUpzqhobLQURdz3DUJZp3slhBATipnNZBVOR4dA1f65awhqatO96jVgB6YPvhr4zW+Anp5InqNSQrebQKVBCClrcpfRALNcG25bgjPpGgw9oNraeisKDF2J73/1QTTvvgf4yleA668vWl63tLOF5l4vR6g0CCEVQxDbgNdb/ObNlvJ5f+d23PHEY7jq/vOBFSvw6H1XoP2fT7AqLVgAnH8+8Nxzoch35pn5fXgqJse5ibW8kgp3TxHSfwmyBTdI3Uwmo8fefqx+6MKrrB1OyGhKVmkbzlU9+WTVX/1KdffuouTz2z1VDluBYbh7ioZwQkjFUFVlDam5iFjLV06CGtFnz+nCt1sGY9fOfQswg2v3YPaBP0Lzhv8ETj4Z+MtfQpPPSTkYymkIJ4T0O4LYBoIa0a+5sqGXwgCAXd11aB38c+jvfw/84AfWwd27gcmTgT/+sY+GKNR2UUmGcioNQkjFEDS+VxBfFM+Be7WgddCf8JX0/Uhn0sArrwC//S1w6qnARz4C3HwzsGVLQfJlqSRDOZUGIaRi8Jo9AMUbkb0GaFXg9uYfYfPicaiuqgZOPBHr33weuOceYMgQ4NJLgUMOAVasKHiLcJjBLiPHxPBRSYWGcEIGFmEZkfPlGMm22bGlQwdfM1jvbL/TuvC551SvuEI1k7G+33ij6qxZqtu3B75/KcOMgIZwQshAIEwj8rx5ln+HW3vZNl98Ywtu/MuNmHzCZKSGpbByy0p0dXfh6JFHW/rlk58E/vxn4IADgK9/HZg61VrGKnNoCCeEDAjCNCJnbSBu+TaybQ4bPAw/Pf2nSA2zknNc/fTV+Njsj2Hb7m3Whc88A/zpT8D48dba1Ec/CtxwQ2BZytVvg0qDEFLRRGFEDtLmjM/NQEvdMzjmw0NQVQUMH7UNN7zcCMydC6xdC8yYAXz+81blxYuBiy4ClizZu/PKTTmUdYBDkzWsSiq0aRAysIjCMc7LvuGWsMmtbu2g3e73nz1btb7eqnTMMdo2cYE21Gf6yL7//u52lShDuIP5NAghA4UojMi5Gf68FJKfF/iidxbpxIcn6sYdG/ddsGWL6h13qDY1aQrveBreC82KWCimSoOGcEII8cDEyO7nBX7HszNxw19uwEtTX0J9bT26092ora7dW6eqSqHqYUBxIUoP8bI2hIvIcBF5XESW2z/7Bri36iVF5A8i8rqIvCYijfFKSggZyJgY2f3sH1NOnILXLn4N9bX1UFWccvcp+PFTP3bUMVcYQHkEOCyVIXwagIWqegSAhfZ3N+4DcIOqHgXgJAAbYpKPEFJiymH3kIlBPJ9jXnZmsatnF04ZewqOHHEkAKAn04PvtG7sc62I++pPYngGzedaAaxKaig3WcMKuwBYBmCU/XkUgGUudY4G8KegbdOmQUjlUw5RX4PIUYhN5b4X79Oaq2v0mlvf7nVtS4uLYb1WNVG/XQVpTQ19TxPDukM3lKOcDeEAtuR8f8+lzhcA/A+AhwG8AOAGANUe7U0B0A6gPZlMFt5rhJCyIEhY86iJylN7zdY1es3T12g6k1ZV1afeeUpXvreyzz0TCdW6uty+yLj2TzGGclOlEZkhXESeAPABl1OtAOao6jBH3fdUtZddQ0TOAnAXgOMBdAB4EMCjqnqX331pCCek8ik0xHilktEMjrjlCKSGpvDkpCd7nfMyxruRSipWrgpmJ8lSckO4qn5WVT/qUn4HYL2IjLIFHQV3W8UaAC+o6tuq2gPgvwGcEJW8hJDyoZKivmbxctIzsctUSRUWTVqEW864BQCwbfc2fO+x72Ht+2t9PNt7a9UG7MD0H2wO52F8KJUhfD6ASfbnSQB+51LnWQAHishI+/vpAF6LQTZCSImpqKivcDdM/+u/At/4hrmxeuzQsfjIQVaMqj91/Am3LrkVa7et9VSUiYTsi6abVMy8ci2av52I6AkdmKxhhV0AJGDtmlpu/xxuH28CMNtR73MAXgLwMoB7AdTla5uGcEL6B6WO+hoELxtMIalfs6zfvl5VrXO1g3b3sV042woDlNqmUSpo0yCExI2XDcaLhgb3NLSAFWW3o8Naips+3QqieMYP5+KZe8/Ajo0jINL7Xn4pbINgatOg0iCEkCIJYqyurgbS6b7HEwlg507vnOaqikMPlchyiZfcEE4IIf0Zp5F7+3agrq73+dravscaGtwVBgB0dvZWGID1/dJLrftUV7srDADo6Ijv5Z9KgxBCApJr+O7stH4mEvvSvN5zD3D33X1Tv6ZSwe7V2bnvPl7I0DXY1bOruIcypCaWuxBCSD+itbXvrKC7G9h/f2DTpt7H3WwNU6b0vb6qqjAflMH1GUyetgaDa8YGv7gAONMghJCAmAQy9PLRaG62ZhyJnN2xhSiMRAKYPasKt/zo5OAXFwiVBiGkIilFQMPsPb2WirI+FfkCCjY3W7MSN6qr9y1n5SqWXHbu7C1XLH1hsi+3kgr9NAjp/5QioKFXNj+3+/v5bWR9K7K+Fn7xo/LdM5tNMIy+AP00CCH9FZPkSHHdM3vfrE8FkN9vo6EBqK+3jNxubTmfYd48y4ZiuqXXq518cMstIaTfYmJTiOueItbg7DR454uRlTWCm4RKaW622g+66yqqvqDSIIRUHKUIaBjknm6xs3LZvHnfFlznllwvz26veFxedo+o+oJKgxBScZQioGGQe2Z3SPnNDpLJfbOITKbvbMWvTaeSuemmmPvCxPBRSYWGcEIGBqUIaJh7z5aW/DLEYbQPoy9AQzghhERHdlutV6yo3LpugQjLCQYsJISQCCnFDq4o4e4pQgiJkFLs4CoHqDQIIaQAKjElbRhQaRBCSAFUWkrasKDSIISQAvDaAltuBu6wYWh0QggpkObm/q8kcuFMgxBCiDFUGoQQQoyh0iCEEGIMlQYhhBBjqDQIIaSCiTuDIXdPEUJIhZIb/yqbVhaIbldXSWYaIjJcRB4XkeX2zwM96s0QkVdF5HURuVlEJG5ZCSGkXGlt7R0wEbC+t7ZGd89SLU9NA7BQVY8AsND+3gsR+QSAUwAcC+CjAP4ewKlxCkkIIeVMKeJflUppTAAwx/48B8AXXOoogMEA6gAMAlALYH0s0hFCSAVQivhXpVIaB6vqOgCwfx6UW0FV/wrgKQDr7PKYqr7u1piITBGRdhFp37hxY4RiE0JI+VCK+FeRKQ0ReUJEXnEpEwyvPxzAUQDGABgN4HQR+ZRbXVWdqapNqto0cuTI8B6CEELKmFLEv4ps95SqftbrnIisF5FRqrpOREYB2OBS7YsA/qaq2+1rFgD4OIA/RiIwIYRUIHHHvyrV8tR8AJPsz5MA/M6lTgeAU0WkRkRqYRnBXZenCCGExEOplMZ1AD4nIssBfM7+DhFpEpHZdp3fAHgLwMsAlgJYqqqPlEJYQgghFiVx7lPVTgCfcTneDmCy/TkN4KKYRSOEEOIDw4gQQggxhkqDEEKIMaKqpZYhVERkI4BVRTQxAsCmkMQJE8oVDMoVDMoVjP4oV0pV8/os9DulUSwi0q6qTaWWIxfKFQzKFQzKFYyBLBeXpwghhBhDpUEIIcQYKo2+zCy1AB5QrmBQrmBQrmAMWLlo0yCEEGIMZxqEEEKMGZBKQ0S+YmcEzIiI504DERknIstEZIWITHMcP1REFtuZBx8UkbqQ5Mqb0VBEThORFx1ll4h8wT53r4i84zh3XFxy2fXSjnvPdxwvZX8dJyJ/tX/fL4nIVx3nQusvr78Vx/lB9rOvsPui0XHuMvv4MhH5fKEyFCjX90TkNbtvFopIynHO9fcZo2zni8hGhwyTHecm2b/35SIyKffaCGX6hUOeN0Vki+NcZP0lIneLyAYRecXjvIiV3XSF/bs8wXEu3L5S1QFXYIVc/zCARQCaPOpUw4p9dRisRFBLARxtn3sIwDn25zsAtIQk1wwA0+zP0wBcn6f+cACbATTY3+8FcFYE/WUkF4DtHsdL1l8APgTgCPvzIbByswwLs7/8/lYcdS4GcIf9+RwAD9qfj7brDwJwqN1OdUj9YyLXaY6/n5asXH6/zxhlOx/ArS7XDgfwtv3zQPvzgXHIlFP/2wDujqm/PgXgBACveJw/E8ACAAIrGvjiqPpqQM40VPV1VV2Wp9pJAFao6tuqugfAAwAmiIgAOB1WQEXAO/NgIZhkNHRyFoAFqtqVp16xBJVrL6XuL1V9U1WX25/fhRWGP+ykK65/Kz6y/gbAZ+y+mQDgAVXdrarvAFhhtxeLXKr6lOPv52+w8tfEgUmfefF5AI+r6mZVfQ/A4wDGlUCmcwHcH8J986Kqf4T1gujFBAD3qcXfAAwTK+1E6H01IJWGIaMBrHZ8X2MfSwDYoqo9OcfDIG9GwxzOQd8/2un29PQXIjIoZrkGi5VB8W/ZJTOUUX+JyEmw3iDfchwOo7+8/lZc69h9sRVW35hcWyhB274A1ttqFrffZ1iYyvZl+/fzGxEZG/DaqGSCvYx3KIAnHYej7K98eMkeel+VJMptHIjIEwA+4HKqVVXd8nf0acLlmPocL1ou0zbsdkYBOAbAY47DlwH4P1gD40wAPwJwdYxyJVX1XRE5DMCTIvIygPdd6pWqv+YCmKSqGftwwf2V27zLsdxnjOTvKQ/GbYvIeQCaYOWtydLn96mqb7ldH5FsjwC4X1V3i8hUWDO10w2vjUqmLOcA+I1a0bizRNlf+Yjt76vfKg31yRxoyBoAYx3fxwB4F1Zcl2EiUmO/MWaPFy2XmGU0zHI2gP9S1W5H2+vsj7tF5B4AP4hTLnv5B6r6togsAnA8gN+ixP0lIgcA+D2Af7en7tm2C+6vHLz+VtzqrBGRGgBDYS03mFxbKEZti8hnYSnhU1V1d/a4x+8zrEEwr2xqpVDIMgvA9Y5rP51z7aI4ZHJwDoBvOg9E3F/58JI99L7i8pQ3zwI4QqydP3Ww/kjmq2VdegqWPQHwzjxYCCYZDbP0WU+1B86sHeELAFx3WkQhl4gcmF3eEZERAE4B8Fqp+8v+3f0XrPXeX+ecC6u/XP9WfGQ9C8CTdt/MB3COWLurDgVwBIAlBcoRWC4ROR7AnQDGq+oGx3HX32dIcpnKNsrxdTz2Ze58DMA/2jIeCOAf0XvGHZlMtlwfhmVU/qvjWNT9lY/5AL5u76L6OICt9ktR+H0VlbW/nAus/ONrAOwGsB7AY/bxQwA86qh3JoA3Yb0ttDqOHwbrH3sFgF8DGBSSXAkACwEst38Ot483AZjtqNcIYC2Aqpzrn4SV6fAVAG0A9o9LLgCfwL4siy8DuKAc+gvAeQC6AbzoKMeF3V9ufyuwlrrG258H28++wu6LwxzXttrXLQNwRsh/6/nkesL+H8j2zfx8v88YZbsWwKu2DE8BONJx7TfsvlwB4F/jksn+fhWA63Kui7S/YL0grrP/ltfAsj9NBTDVPi8AbsO+bKdNjmtD7St6hBNCCDGGy1OEEEKModIghBBiDJUGIYQQY6g0CCGEGEOlQQghxBgqDUKKQES2R9Bmo4h8Lex2CQkDKg1Cyo9GAFQapCyh0iAkBETk0yKyyA6s94aIzLM9zSEiK0XkehFZYpfD7eP3ishZjjays5brAHxSrLwM343/aQjxhkqDkPA4HsB3YOXIOAxWKIks76vqSQBuBfCfedqZBuAZVT1OVX8RiaSEFAiVBiHhsURV16gVRfdFWMtMWe53/Dw5bsEICQsqDULCY7fjcxq9o0iry+ce2P+D9lJWKGlwCYkSKg1C4uGrjp/Z6KgrAZxof54AoNb+vA3AkNgkIyQA/TafBiFlxiARWQzrRe1c+9gsAL8TkSWwovTusI+/BKBHRJYCuJd2DVJOMMotIREjIithhareVGpZCCkWLk8RQggxhjMNQgghxnCmQQghxBgqDUIIIcZQaRBCCDGGSoMQQogxVBqEEEKModIghBBizP8DOoteau/r0EwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b758471a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training data,\n",
    "# as well as out model and the true model\n",
    "plt.plot(s,y, 'r--')\n",
    "plt.plot(s,p, 'g:')\n",
    "plt.plot(train[0], train[1], 'bo')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
